{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "### 8/10 perfect solutions required for full score\n",
    "\n",
    "These exercises should be done on a computer using Python. \tPlease submit a Jupyter-notebook, containing both your code and the report in the same file. Provide properly commented code.\n",
    "\n",
    "Some of the tasks involve sparse matrices, to obtain efficient code it is preferable to work with sparse matrices using for example the library `sparse` of  `scipy`. In these exercises you are allowed to work with full matrices if you find it easier, but make sure that the comparisons of the various methods in your implementations, e.g. when comparing running times, are fair. Be explicit about these issues in your report.\n",
    "\n",
    "**You will need an additional Python library for this project:** The library `autograd` will be used to compute the numerical Jacobian. You can install this with `pip install autograd`, `pip3 install autograd` or `conda install autograd` depending on your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import matplotlib.pyplot as plt\n",
    "import decimal as dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Propagation of rounding errors\n",
    "The following exercise illustrates the effects of propagation of rounding errors in a numerical algorithm.\n",
    "\n",
    " \n",
    "We want to make a program to approximate the derivative of a differentiable function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$. We use the following approximation called \"central difference\"\n",
    "\n",
    "$$f'(x)=\\frac{f(x+\\Delta x/2)-f(x-\\Delta x/2)}{\\Delta x}+\\mathcal{O}((\\Delta x)^2),\\qquad f'(x)\\approx \\frac{f(x+\\Delta x/2)-f(x-\\Delta x/2)}{\\Delta x}.$$\n",
    "\n",
    "1. Use a Taylor expansion to prove that the error given by this approximation is indeed $\\mathcal{O}((\\Delta x)^2)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here ...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take $f(x)=\\exp(-x^2)$, and use the formula above to approximate \n",
    "\n",
    "$$\\left.\\frac{d}{dx}\\exp (-x^2)\\right|_{x=\\frac{\\pi}{2}}.$$\n",
    "\n",
    "2. Make a simple Python program to compute the absolute error of the approximation for decreasing values of $\\Delta x$, e.g $\\Delta x=\\frac{1}{2^k}$ for $k=1,2,\\dots , N$, choosing a value of $N$ between $30$ and $40$.\n",
    "Plot the results (values of $\\Delta x$ versus the corresponding absolute error) in a logarithmic scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see in your plot that for very small values of $\\Delta x$ the error starts to increase, compromising the convergence of the approximation method. This is due to the propagation of rounding error. The results improve if we can compute our formulae with a higher precision.\n",
    "\n",
    "3. In this exercise you should learn how to use the library `decimal` in Python in order to perform the calculations of the previous experiment with higher precision. \n",
    "Consider the same parameters used in the previous experiment ($\\Delta x=\\frac{1}{2^k}$ for $k=1,2,\\dots , N$ and same $N$ as above), increase the level of precision (look up `getcontext().prec` in the `decimal` library) and produce a plot which does not show the effects of propagation of rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Newton's method\n",
    "\n",
    "In this exercise, we will make use of the `autograd` package. As we have done at the start of this noteobook, when using this package you must `import` Numpy using `autograd.numpy`. We use the `jacobian` function, which approximates the jacobian of functions between Numpy arrays. See the following example.\n",
    "\n",
    "$$ F(x_1, x_2) = (x_1 + x_2, x_1x_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "def F_0(x):\n",
    "    return np.array([x[0] + x[1], x[1]*x[0]])\n",
    "\n",
    "# Computing the numerical Jacobian\n",
    "DF = jacobian(F_0)\n",
    "# Testing our function\n",
    "x = np.array([3.0, 1.0])\n",
    "print(DF(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. You are given the following function:\n",
    "$$\n",
    "    F(x_1, x_2) = (x_1 + x_1x_2 - 1, x_1^2 + x_2 + 3)\n",
    "$$\n",
    "  - Using the package `autograd` for automatic differentiation to compute Jacobians, create a function `newton` which implements Newton's method for systems of nonlinear equations $F(\\mathbf{x})=0$.\n",
    "  - Apply the method to $F$. These numerical experiments should give evidence that Newton's method converges for these two problems.\n",
    "  - Choose an initial value close enough in $\\infty$-norm to one solution of the system. Plot the values of $\\|\\mathbf{x}^{k+1}-\\mathbf{x}^k\\|_{\\infty}$ and $\\|F(\\mathbf{x}_k)\\|_{\\infty}$ for each iteration  $k=0,1,\\dots, K-1$ where $K$ is the total number of iterations (choose $K$ large enough to make a detailed plot). Use a semi-logarithmic scale on the y-axis (`semilogy` in matplotlib). \n",
    "  - Your answer is your code and a pair of figures with comments in text about what the figures show and what they can tell you about the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Newton's method implementation\n",
    "def newton(F, DF, x0, max_iterasjoner):\n",
    "    xn = x0\n",
    "    for n in range(0, maks_iterasjoner):\n",
    "        if abs(xn0 - xn) < 0.001:\n",
    "            print(\"Funnet svar etter\", n, \"iterasjoner.\")\n",
    "            return xn\n",
    "        if DF(xn) == 0:\n",
    "            print(\"Jacobimatrisen er lik null. Ingen lÃ¸sning funnet\")\n",
    "            return None\n",
    "        xn0 = xn\n",
    "        xn = xn - (1/(DF(xn)))*F(xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the method to F\n",
    "def F_0(x):\n",
    "    return np.array([x[0] + x[1], x[1]*x[0]])\n",
    "\n",
    "# Computing the numerical Jacobian\n",
    "DF = jacobian(F_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the implemented Newton method and implement a stopping criterion.\n",
    "As stopping criterion, you should use a maximum number of iterations as well as two tolerances $TOL_1$ and $TOL_2$ such that the iteration is stopped whenever $\\|F(\\mathbf{x}^k)\\|_{\\infty}\\le TOL_1$ and $\\|\\mathbf{x}^{k+1}-\\mathbf{x}^k\\|_{\\infty}\\le TOL_2$. \n",
    "Provide numerical evidence that your code works as it should by  printing the values $\\|F(\\mathbf{x}^k)\\|_{\\infty}$ for each iteration , and making a plot of $\\|F(\\mathbf{x}^k)\\|_{\\infty}$, $\\|\\mathbf{x}^{k+1}-\\mathbf{x}^k\\|_{\\infty}$, for all $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your method with a stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show your evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use newtons method to make a function that calculates a n-th root of a matrix $A$. That is, a function that solves $X^n = A$.\n",
    "\n",
    "- Confirm numerically that your implementation is correct.\n",
    "\n",
    "**Hint**. Can you reinterpret the function $F(X) = X^n - A$ as a function between arrays? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Think of a good way of verifying numerically that the convergence of the Newton iteration is quadratic. \n",
    "- Your answer is the code that does the verification, the evidence it produces and an explanation of why it is evidence for quadratic convergence.\n",
    "\n",
    "**Hint**. You should consider a problem for which you know the solution or alternatively generate a reference solution (i.e. an approximation with *very* high accuracy). Use the definition of quadratic convergence. You can also compute the jacobian exactly (by hand) to eliminate the error from that part of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Linear systems -Poisson equation-\n",
    "We are going to numerically solve the following equation for $(x,y)\\in [0,1]^2$ with the Dirichlet boundary condition:\n",
    "\t$$\n",
    "\tu_{xx} + u_{yy}= f, \\text{and} \\;\\; u=0 {\\text{ for boundary}} \n",
    "\t$$\n",
    "    where $u_{xx}$ and $u_{yy}$ are the second order partial derivatives of the unknown function $u(x,y)$, and $f(x,y)$ is a given function. We discretize the above problem on the regular equispaced grid: $x_i=\\frac{i}{n+1}$,  and $y_j=\\frac{j}{n+1}$  with $i,j=1,...,n$ ($0$ and $n+1$ correspond to the boundary). \n",
    "    We use the following finite difference approximation\n",
    "    $$u_{xx}(x_i,y_j)+u_{yy}(x_i,y_j)\\approx\\frac{u(x_{i+1},y_j)-2u(x_{i},y_j)+u(x_{i-1},y_j)}{(\\Delta x)^2} +\\frac{u(x_i,y_{j+1})-2u(x_{i},y_{j})+u(x_i,y_{j-1})}{(\\Delta y)^2}.$$\n",
    "Note that we have $\\Delta x=\\Delta y=1/(n+1)$ here. Thus we consider the following linear system to approximate the above problem \n",
    "\t$$\n",
    "\t\\frac{1}{(\\Delta x)^2}A \\mathbf{u}  = \\mathbf{f},\n",
    "\t$$\n",
    "where the left hand side is a finite-difference discretization of $u_{xx} + u_{yy}$, the matrix $A$ is implemented in the code below, and $\\mathbf{f}$ is the discretized vector of function $\\mathbf{f}=(f(x_1,y_1),f(x_2,y_1),...,f(x_n,y_1),f(x_1,y_2),...,f(x_n,y_2),....,f(x_n,y_n))^{\\top}$, as well as $\\mathbf{u}=(u(x_1,y_1),u(x_2,y_1),...,u(x_n,y_1),u(x_1,y_2),...,u(x_n,y_2),....,u(x_n,y_n))^{\\top}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.  1.  0. ...  0.  0.  0.]\n",
      " [ 1. -4.  1. ...  0.  0.  0.]\n",
      " [ 0.  1. -4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ... -4.  1.  0.]\n",
      " [ 0.  0.  0. ...  1. -4.  1.]\n",
      " [ 0.  0.  0. ...  0.  1. -4.]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "# Creating the matrix A.\n",
    "nx, ny = 10, 10\n",
    "m = nx*ny # number of x_i, y_i pairs\n",
    "main_diag = np.ones(m)*-4.0\n",
    "side_diag = np.ones(m-1)\n",
    "side_diag[np.arange(1,m)%nx==0] = 0\n",
    "side_diag2 = np.ones(m-nx)\n",
    "diagonals = [main_diag,side_diag,side_diag,side_diag2,side_diag2]\n",
    "Amatrix = sparse.diags(diagonals, [0, -1, 1,nx,-(nx)])\n",
    "\n",
    "print(Amatrix @ np.identity(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the linear system \n",
    "\t$$\n",
    "\tA \\mathbf{u}  = (\\Delta x)^2 \\mathbf{f},\n",
    "\t$$\n",
    "    in the following.\n",
    "For convenience, we introduce the following notation:\n",
    "\n",
    "- $A_d$ the diagonal part of $A$,\n",
    "- $A_l$ is **minus** the strictly lower-triangular part of $A$,\n",
    "- $A_u$ is **minus** the strictly upper-triangular part of $A$,\n",
    "\n",
    "with this notation, $A = A_d - A_l - A_u$.\n",
    "\n",
    "We will now approximate the solution of this linear system using three different iterative methods of the type \n",
    "\t$$\n",
    "\t\tM \\mathbf{u}^{(k+1)} = N \\mathbf{u}^{(k)}+(\\Delta x)^2\\mathbf{f},\n",
    "\t$$\n",
    "\twhere $A = M - N$ with $|M|\\ne0$. These methods are aslo known as splitting methods and  each choice of $M$ (and $N=M-A$) determines a different method.  We will use the following three iterative methods to calculate approximate solutions to $\\mathbf{u}$:\n",
    "\n",
    "a) Jacobi ($M = A_d$).\n",
    "\n",
    "b) Forward Gauss-Seidel ($M = A_d-A_l$).\n",
    "\n",
    "c) Successive over relaxation ($M = A_d-\\omega A_l$, where you can choose the value of $\\omega$).\n",
    "\n",
    "1. Create a function that implements the above iterative method. This function should take as input the matrix `A` and `M` such that you can use this function for all three methods above, and outputs the solution to the linear system `u`. Use the three methods to find the solution to the linear system with `n=nx=ny=10`, the initial guess $\\mathbf{u}^{(0)}=(1,1,...,1)^\\top\n",
    "$, and the function $f(x,y)=\\exp(-10(x-1/2)^2-10(y-1/2)^2)$. Confirm that what you obtain is indeed a solution numerically. Do not forget to multiply $(\\Delta x)^2$ with $\\mathbf{f}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code - Create the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to solve the problem with each of the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider the residual vector $\\mathbf{r}^k:=\\mathbf{f}-A\\mathbf{u}^k$. We wish to compare the speed of convergence for each of the methods above. \n",
    "   - Produce a semi-log plot of the  $2$-norm of the relative residual  $\\frac{\\|\\mathbf{r}^k\\|_2}{\\|\\mathbf{r}^0\\|_2}$ versus the number of iterations. Plot all the methods in the same plot for comparison. \n",
    "   - Try choosing different values of $\\omega$ in the SOR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
